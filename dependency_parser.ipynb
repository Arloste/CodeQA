{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tags_.json\", 'r') as f:\n",
    "    data = [\n",
    "        json.loads(x) for x in f.readlines()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# structure:\n",
    "# path to parent folder, name, contents\n",
    "plain_text_files = list()\n",
    "plain_text_file_path = \"test_input/test_repo_/\"\n",
    "\n",
    "file_types_to_consider = [\"txt\", 'md', 'yml']\n",
    "\n",
    "def walk_dir(path, ind=0):\n",
    "    for name in os.listdir(path):\n",
    "        if os.path.isdir(path + name + '/'):\n",
    "            walk_dir(path + name + '/', ind+4)\n",
    "        else:\n",
    "            ext = name.split('.')[-1]\n",
    "            if ext in file_types_to_consider:\n",
    "                with open(path + name, 'r', encoding=\"utf-8\") as f:\n",
    "                    contents = f.read()\n",
    "                    full_path = path + name\n",
    "                plain_text_files.append(\n",
    "                    (full_path[len(plain_text_file_path):], contents)\n",
    "                )\n",
    "                \n",
    "\n",
    "walk_dir(plain_text_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list unique files\n",
    "\n",
    "unique_files = set()\n",
    "for item in data:\n",
    "    unique_files.add(item['rel_fname'])\n",
    "for item in plain_text_files:\n",
    "    unique_files.add(item[0])\n",
    "\n",
    "unique_files = [\"root/\"+x for x in list(unique_files)]\n",
    "unique_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_csv = {\"root\": 0}\n",
    "edges_csv = set()\n",
    "\n",
    "def extract_relationships(file_paths):\n",
    "    \"\"\"\n",
    "    Extracts unique relationships from a list of file paths.\n",
    "\n",
    "    Args:\n",
    "    - file_paths (list of str): A list of file paths.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list of relationships in the format \"folder contains folder\" or \"folder contains file\".\n",
    "    \"\"\"\n",
    "    relationships = set()\n",
    "\n",
    "    for path in file_paths:\n",
    "        # Split the path into components\n",
    "        parts = path.split('/')\n",
    "\n",
    "        # Generate relationships\n",
    "        for i in range(1, len(parts)):\n",
    "            start = 1 if i > 1 else 0\n",
    "            # Construct the current folder path\n",
    "            current_folder = '/'.join(parts[start:i])\n",
    "            # Construct the next part (either a folder or a file)\n",
    "            next_part = '/'.join(parts[1:i+1])\n",
    "            \n",
    "            for folder in [current_folder, next_part]:\n",
    "                if folder not in nodes_csv:\n",
    "                    nodes_csv[folder] = len(nodes_csv)\n",
    "\n",
    "            relation = \"folder\" if i < len(parts) - 1 else \"file\"\n",
    "            relationships.add(f\"{current_folder} contains {relation} {next_part}\")\n",
    "            edges_csv.add((\n",
    "                nodes_csv[current_folder], f\"contains {relation}\", nodes_csv[next_part]\n",
    "            ))\n",
    "\n",
    "    return list(relationships)\n",
    "\n",
    "\n",
    "relationships = extract_relationships(unique_files)\n",
    "_=[print(x) for x in sorted(list(relationships))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All def links\n",
    "defs = list()\n",
    "defs_dict = dict()\n",
    "\n",
    "for item in data:\n",
    "    if item['kind'] == \"ref\": continue\n",
    "    \"\"\"f\n",
    "    This thing also provides those relations\n",
    "    file contains function\n",
    "    file contains class\n",
    "    class contains method\n",
    "    \"\"\"\n",
    "    defs.append((\n",
    "        item['fname'], item['name'], item['info']\n",
    "    ))\n",
    "    defs_dict[\n",
    "        (item['fname'], item['name'].split()[-1])\n",
    "    ] = len(defs_dict)\n",
    "\n",
    "    loc = (item['rel_fname'], item['name'])\n",
    "    if loc not in nodes_csv:\n",
    "        nodes_csv[loc] = len(nodes_csv)\n",
    "\n",
    "    print(defs[-1][:-1], len(defs_dict)-1)\n",
    "\n",
    "    if item['category'] == \"function\" and '.' not in item['name']:\n",
    "        edges_csv.add((\n",
    "            nodes_csv[item['rel_fname']], \"file contains function\", nodes_csv[loc]\n",
    "        ))\n",
    "    \n",
    "    elif item['category'] == \"class\":\n",
    "        edges_csv.add((\n",
    "            nodes_csv[item['rel_fname']], \"file contains class\", nodes_csv[loc]\n",
    "        ))\n",
    "        for method in item['info'].split('\\n'):\n",
    "            method_loc = (item['rel_fname'], method)\n",
    "            if method_loc not in nodes_csv:\n",
    "                nodes_csv[method_loc] = len(nodes_csv)\n",
    "            edges_csv.add((\n",
    "                nodes_csv[loc], \"class contains method\", nodes_csv[method_loc]\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "def parse_imports(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        tree = ast.parse(file.read())\n",
    "\n",
    "    imports = {}\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for alias in node.names:\n",
    "                imports[alias.asname or alias.name] = alias.name\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            module = node.module\n",
    "            for alias in node.names:\n",
    "                imports[alias.asname or alias.name] = f\"{module}.{alias.name}\"\n",
    "\n",
    "    return imports\n",
    "\n",
    "def find_imported_function_location(file_path, function_name):\n",
    "    # Parse the imports in the file\n",
    "    imports = parse_imports(file_path)\n",
    "\n",
    "    # Get the base directory of the file\n",
    "    base_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Check if the function is in the imports dictionary\n",
    "    if function_name in imports:\n",
    "        import_path = imports[function_name]\n",
    "        print(file_path, base_dir)\n",
    "        print(import_path)\n",
    "        # Resolve the import path to a file path\n",
    "        parts = (parts := import_path.split('.'))[len(parts)-2:-1]\n",
    "        resolved_path = os.path.join(base_dir, *parts) + '.py'\n",
    "        return resolved_path\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_local_path(file_path, function_name):\n",
    "    for item in data:\n",
    "        if item['kind'] == 'def' and item['fname'] == file_path and item['name'].split('.')[-1] == function_name:\n",
    "            return file_path\n",
    "    \n",
    "    return \"\"\n",
    "    \n",
    "def resolve_reference(file_path, function_name):\n",
    "    if location := get_local_path(file_path, function_name) or find_imported_function_location(file_path, function_name):\n",
    "        # return location\n",
    "        return defs_dict.get(\n",
    "            (location, function_name), -1\n",
    "        )\n",
    "    return -1\n",
    "\n",
    "\n",
    "for item in data:\n",
    "    if item['kind'] == \"def\": continue\n",
    "    if item['rel_fname'] != \"tests/test_utils.py\" or item['name'] != \"get_python_paths_list\": continue\n",
    "    # print(item['rel_fname'], item['name'])\n",
    "    ref = resolve_reference(item['fname'], item['name'])\n",
    "    # print(item['rel_fname'], item['name'], ref)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import builtins\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import builtins\n",
    "\n",
    "def resolve_reference(name, file_path, project_root):\n",
    "    \"\"\"\n",
    "    Resolves the reference of a function, class, or method to its definition.\n",
    "\n",
    "    Args:\n",
    "    - name (str): The name of the function, class, or method to resolve.\n",
    "    - file_path (str): The relative file path where the name is referenced, starting from the project root.\n",
    "    - project_root (str): The root directory of the project.\n",
    "\n",
    "    Returns:\n",
    "    - str: The file path of the definition, or an empty string if it's a built-in or external module.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the file to analyze imports and definitions\n",
    "    with open(os.path.join(project_root, file_path), 'r') as file:\n",
    "        tree = ast.parse(file.read())\n",
    "\n",
    "    # Track imports\n",
    "    imports = {}\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for alias in node.names:\n",
    "                imports[alias.asname or alias.name] = alias.name\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            module = node.module\n",
    "            for alias in node.names:\n",
    "                imports[alias.asname or alias.name] = f\"{module}.{alias.name}\"\n",
    "\n",
    "    # Check if the name is imported\n",
    "    if name in imports:\n",
    "        import_path = imports[name]\n",
    "        if '.' in import_path:\n",
    "            # Handle nested imports\n",
    "            module_parts = import_path.split('.')\n",
    "            module_file = os.path.join(project_root, *module_parts[:-1], f\"{module_parts[-2]}.py\")\n",
    "            if os.path.exists(module_file):\n",
    "                return module_file\n",
    "        else:\n",
    "            # Handle top-level imports\n",
    "            module_file = os.path.join(project_root, f\"{import_path}.py\")\n",
    "            if os.path.exists(module_file):\n",
    "                return module_file\n",
    "\n",
    "    # Search the project directory for the definition\n",
    "    for root, dirs, files in os.walk(project_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                module_file_path = os.path.join(root, file)\n",
    "                with open(module_file_path, 'r') as f:\n",
    "                    module_tree = ast.parse(f.read())\n",
    "                    for node in ast.walk(module_tree):\n",
    "                        if isinstance(node, ast.FunctionDef) or isinstance(node, ast.ClassDef):\n",
    "                            if node.name == name:\n",
    "                                return module_file_path\n",
    "\n",
    "    # If the name is not found, assume it's an external module\n",
    "    return \"No location detected\"\n",
    "\n",
    "project_root = 'test_input/codegraph-main'\n",
    "\n",
    "current_ref = None\n",
    "for i, item in enumerate(data):\n",
    "    if item['kind'] == \"def\":\n",
    "        current_ref = item.copy()\n",
    "        continue\n",
    "\n",
    "    name = item['name']\n",
    "    file_path = item['rel_fname']\n",
    "\n",
    "    definition_location = resolve_reference(name, file_path, project_root)\n",
    "    definition_location = definition_location[len(project_root)+1:]\n",
    "\n",
    "    location_id = nodes_csv.get((definition_location, name), -1)\n",
    "    \n",
    "    if location_id == -1: continue\n",
    "\n",
    "    edges_csv.add((\n",
    "        nodes_csv[(current_ref['rel_fname'], current_ref['name'])],\n",
    "        \"references\", location_id\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text_dict = {v[0]: v[1] for v in plain_text_files}\n",
    "plain_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    loc = (item['rel_fname'], item['name'])\n",
    "    node_id = nodes_csv.get(loc, -1)\n",
    "    if node_id == -1 or item['kind'] == \"ref\": continue\n",
    "    del nodes_csv[loc]\n",
    "    nodes_csv[node_id] = f\"# location: {item['rel_fname']}\\n{item['info']}\"\n",
    "\n",
    "nodes_csv_items = nodes_csv.copy().items()\n",
    "for k, v in nodes_csv_items:\n",
    "    if type(v) == int:\n",
    "        del nodes_csv[k]\n",
    "        nodes_csv[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_csv_items = nodes_csv.copy().items()\n",
    "for node_id, node_location in nodes_csv_items:\n",
    "    if text := plain_text_dict.get(node_location, \"\"):\n",
    "        nodes_csv[node_id] = f\"{node_location} file contents: {text}\"\n",
    "\n",
    "nodes_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [(k, v) for k, v in nodes_csv.items()]\n",
    "nodes = sorted(nodes, key=lambda l:l[0])\n",
    "\n",
    "with open(\"a\", 'w') as f:\n",
    "    f.write(\"id,name\\n\")\n",
    "    for item in nodes:\n",
    "        f.write(f\"{item[0]},{item[1]}\\n\")\n",
    "\n",
    "# with open(\"a\", 'w') as f:\n",
    "#     f.write(\"id_head,type,id_tail\\n\")\n",
    "#     for item in edges_csv:\n",
    "#         f.write(f\"{item[0]},{item[1]},{item[2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"G-Retriever/dataset_/edges.csv\") as f:\n",
    "    edges = f.readlines()[1:]\n",
    "    edges = [x.strip().split(',') for x in edges]\n",
    "    heads, relation_types, tails = zip(*edges)\n",
    "\n",
    "with open(\"G-Retriever/dataset_/nodes.csv\") as f:\n",
    "    nodes = f.readlines()[1:]\n",
    "    nodes = [x.strip().split(',') for x in nodes]\n",
    "    node_ids = [x[0] for x in nodes]\n",
    "    nodes_texts = [','.join(x[1:]) for x in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tails) - set(node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Assuming the folder is named 'my-folder' and it contains a module 'mymodule.py'\n",
    "folder_name = 'G-Retriever'\n",
    "module_name = 'src/utils/lm_modeling'\n",
    "module_path = f\"./{folder_name}/{module_name}.py\"\n",
    "\n",
    "# Load the module\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[module_name] = module\n",
    "spec.loader.exec_module(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inherit model weights from sentence-transformers/all-roberta-large-v1\n"
     ]
    }
   ],
   "source": [
    "# default module name\n",
    "llm_module_name = \"sbert\"\n",
    "\n",
    "model, tokenizer, device = module.load_model[llm_module_name]()\n",
    "text2embedding = module.load_text2embedding[llm_module_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch_geometric.data.data import Data\n",
    "\n",
    "def process_in_batches(elements, batch_size, processing_function, *args):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(elements), batch_size)):\n",
    "        batch_elements = elements[i:i + batch_size]\n",
    "        batch_result = processing_function(*args, batch_elements)\n",
    "        results.append(batch_result)\n",
    "    return torch.cat(results, dim=0)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Process nodes in batches\n",
    "x = process_in_batches(nodes_texts, batch_size, text2embedding, model, tokenizer, device)\n",
    "\n",
    "# Process relation types in batches\n",
    "e = process_in_batches(relation_types, batch_size, text2embedding, model, tokenizer, device)\n",
    "\n",
    "# Create edge index tensor\n",
    "edge_index = torch.LongTensor([\n",
    "    pd.Series(heads).astype(int), pd.Series(tails).astype(int)\n",
    "])\n",
    "\n",
    "# Create graph data structure\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=e, num_nodes=len(nodes_texts))\n",
    "\n",
    "# Save the graph data\n",
    "torch.save(data, 'test_input_/output/graph.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
