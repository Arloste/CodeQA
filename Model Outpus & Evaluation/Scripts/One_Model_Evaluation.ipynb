{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s4GMlupDwJG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "xQLD_7K1lgi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "akG82qok_7uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "id": "Hh8wzmCN_8pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "Tpdz9Arr_9tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/G-Retriever 512 items.jsonl\", 'r') as f:\n",
        "    graph = f.readlines()\n",
        "    graph = [json.loads(x) for x in graph]\n",
        "\n",
        "graph[0]"
      ],
      "metadata": {
        "id": "ER4Tq67sAB7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)**\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Parameters for file output file name\n",
        "type_f = \"RAG\" # Type: RAG or G-Retriever\n",
        "items = 2048 # context length used\n",
        "file_n = 1 # Current run. Must be 3 runs in total\n",
        "\n",
        "\n",
        "PROMPT = \"\"\"You are a Python programming teacher in the SpyderIDE repository. Your task is to score your students' answers on questions.\n",
        "You are given a question from your test, the correct answer for the question, and the student's answer to that question.\n",
        "Score the student's answer based on how correct it is in regards to the correct answer. You are a strict but fair teacher.\n",
        "Only use the provided information; do not reference the SpyderIDE in your explanations.\n",
        "You must provide your answer in the json format like this:\n",
        "{\"explanation\": \"<short analysis of the student's answer>\", \"score\": \"<your score here, an integer from 0 to 10>\"}\n",
        "Your decision must be brief and at most 1 sentence long, because you are a busy teacher and have a lot of answers to grade. Start you explanation with: \"The answer is ...\".\n",
        "Do not rewrite the texts of the answers in your explanations. Do not mention that you are lowering the score.\n",
        "\n",
        "### QUESTION\n",
        "<USER QUESTION>\n",
        "### CORRECT ANSWER\n",
        "<GROUND TRUTH ANSWER>\n",
        "### STUDENT’S ANSWER\n",
        "<STUDENT’S ANSWER>\n",
        "### YOUR DECISION\n",
        "\"\"\"\n",
        "\n",
        "answers = list()\n",
        "\n",
        "for ans in graph:\n",
        "    modified_prompt = PROMPT.replace(\"<USER QUESTION>\", ans['question'])\n",
        "    modified_prompt = modified_prompt.replace(\"<GROUND TRUTH ANSWER>\", ans['label'])\n",
        "    modified_prompt = modified_prompt.replace(\"<STUDENT’S ANSWER>\", ans['pred'])\n",
        "    # print(modified_prompt)\n",
        "\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": modified_prompt},\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    while True:\n",
        "        text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "        print(ans['id'], end=\" \")\n",
        "        _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                        use_cache = True, temperature = 1, min_p = 0.1)\n",
        "        try:\n",
        "            answer = tokenizer.batch_decode(_)[0].split(\"### YOUR DECISION\")[1]\n",
        "            answer = answer[\n",
        "                answer.find('{') : answer.find('}')+1\n",
        "            ]\n",
        "\n",
        "            answer = json.loads(answer)\n",
        "            answer['explanation']\n",
        "            int(answer['score'])\n",
        "            answer['id'] = ans['id']\n",
        "            answers.append(answer)\n",
        "            break\n",
        "        except: pass\n",
        "\n",
        "\n",
        "with open(f'/content/{type_f} {items} items evaluation-{file_n}.jsonl', 'w') as f:\n",
        "    for item in answers:\n",
        "        f.write(\n",
        "            f\"{json.dumps(item)}\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "NFpv9upeADkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
