{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s4GMlupDwJG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "xQLD_7K1lgi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "akG82qok_7uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "id": "Hh8wzmCN_8pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "Tpdz9Arr_9tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/RAG 512 items.jsonl\", 'r') as f:\n",
        "    rag = f.readlines()\n",
        "    rag = [json.loads(x) for x in rag]\n",
        "\n",
        "\n",
        "with open(\"/content/Fine-tuned G-Retriever 512 items.jsonl\", 'r') as f:\n",
        "    graph = f.readlines()\n",
        "    graph = [json.loads(x) for x in graph]"
      ],
      "metadata": {
        "id": "ER4Tq67sAB7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag[0]"
      ],
      "metadata": {
        "id": "cOanAHjLBC1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph[0]"
      ],
      "metadata": {
        "id": "ratK0OaCBCgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)**\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "fname = \"RAG vs Fine-tuned G-Retriever 512 items evaluation.jsonl\"\n",
        "\n",
        "PROMPT = \"\"\"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question and the model solution displayed below.\n",
        "You should choose the assistant, whose answer is the most similar to the model solution below.\n",
        "Your evaluation should consider factors such as the relevance to the user question; accuracy and the lack of hallucinations of their responses in respect to the model answer.\n",
        "Begin your evaluation by comparing the two responses and provide a short explanation.\n",
        "Think step by step. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\n",
        "Your evaluation is independent from the length of the responses.\n",
        "Your evaluation is independent from the names of the assistants. Be as objective as possible. After providing your explanation you must output your final verdict by strictly following this format:\n",
        "\"Assistant A's answer is better\" if assistant A’s answer is better than assistant B’s answer,\n",
        "\"Assistant B's answer is better\" if assistant B’s answer is better than assistant A’s answer,\n",
        "\"Both assistants are good\" if both assistant A and B have provided good results, and\n",
        "\"Both assistants are bad\" if both assistants gave a wrong answer.\n",
        "You must provide your answer in the json format like this:\n",
        "{\"explanation\": \"<short explanation of your verdict here>\", \"verdict\": \"<your verdict here that is based on explanation>\"}\n",
        "Note that you are busy, and you do not have a lot of time for explanation. Do not write more than 1 sentence.\n",
        "### USER QUESTION\n",
        "<USER QUESTION>\n",
        "### MODEL SOLUTION\n",
        "<MODEL SOLUTION>\n",
        "### Assistant A’s answer\n",
        "<ANSWER A>\n",
        "### Assistant B’s answer\n",
        "<ANSWER B>\n",
        "### YOUR VERDICT\"\"\"\n",
        "\n",
        "answers = list()\n",
        "\n",
        "for r, g in zip(rag, graph): # need to switch those on the second run\n",
        "    modified_prompt = PROMPT.replace(\"<USER QUESTION>\", g['question'])\n",
        "    modified_prompt = modified_prompt.replace(\"<MODEL SOLUTION>\", g['label'])\n",
        "    modified_prompt = modified_prompt.replace(\"<ANSWER A>\", r['pred'])\n",
        "    modified_prompt = modified_prompt.replace(\"<ANSWER B>\", g['pred'])\n",
        "    # print(modified_prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": modified_prompt},\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    while True:\n",
        "        print(r['id'], end=\" \")\n",
        "        _ = model.generate(input_ids = inputs, max_new_tokens = 128,\n",
        "                        use_cache = True, temperature = 1.0, min_p = 0.1)\n",
        "        try:\n",
        "            answer = tokenizer.batch_decode(_)[0].split(\"### YOUR VERDICT\")[1]\n",
        "            answer = answer[\n",
        "                answer.find('{') : answer.find('}')+1\n",
        "            ]\n",
        "\n",
        "            answer = json.loads(answer)\n",
        "            answer['explanation']\n",
        "            answer['verdict'] in [\"Assistant A's answer is better\", \"Assistant B's answer is better\", \"Both assistants are good\", \"Both assistants are bad\"]\n",
        "            answer['id'] = r['id']\n",
        "            answers.append(answer)\n",
        "            print(answer['verdict'])\n",
        "            break\n",
        "        except: pass\n",
        "\n",
        "with open(f'/content/{fname}', 'w') as f:\n",
        "    for item in answers:\n",
        "        f.write(\n",
        "            f\"{json.dumps(item)}\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "NFpv9upeADkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}