{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info of One Model evals (without fine-tuned G-Retriever)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "path = \"Model Outpus & Evaluation/One Model Evaluation/\"\n",
    "files_to_data = dict()\n",
    "\n",
    "for f_name in os.listdir(path):\n",
    "    with open(f\"{path}/{f_name}\", 'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(x) for x in data]\n",
    "    \n",
    "    files_to_data[f_name] = data\n",
    "\n",
    "unique_files = [x[:-7] for x in list(files_to_data.keys())]\n",
    "unique_files = list(set(unique_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "rag_df = pd.DataFrame()\n",
    "g_retr_df = pd.DataFrame()\n",
    "ft_df = pd.DataFrame()\n",
    "\n",
    "f_name_to_scores = dict()\n",
    "\n",
    "for f_name in unique_files:\n",
    "    scores_list = list()\n",
    "    for q_id in range(len(files_to_data[f_name+ '1.jsonl'])):\n",
    "        score = list()\n",
    "        for i in range(1, 4):\n",
    "            f_name_with_number = f\"{f_name}{i}.jsonl\"\n",
    "            score.append(\n",
    "                int(files_to_data[f_name_with_number][q_id]['score'])\n",
    "            )\n",
    "        scores_list.append(\n",
    "            sum(score) / 3\n",
    "        )\n",
    "    f_name_to_scores[f_name] = scores_list\n",
    "\n",
    "    count = re.findall(r\"\\d+\", f_name)[0]\n",
    "\n",
    "    if \"FT\" in f_name:\n",
    "        ft_df[str(count)] = scores_list\n",
    "    elif \"RAG\" in f_name:\n",
    "        rag_df[str(count)] = scores_list\n",
    "    else:\n",
    "        g_retr_df[str(count)] = scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vals = list(map(str,sorted(map(int, g_retr_df.columns))))\n",
    "\n",
    "rag_df = rag_df.reindex(sorted_vals, axis=1)\n",
    "g_retr_df = g_retr_df.reindex(sorted_vals, axis=1)\n",
    "\n",
    "rag_df['category'] = ['syntax'] * 140 + ['dependencies'] * 135 + ['meta'] * 50\n",
    "rag_df['type'] = ['RAG']*325\n",
    "\n",
    "g_retr_df['category'] = ['syntax'] * 140 + ['dependencies'] * 135 + ['meta'] * 50\n",
    "g_retr_df['type'] = ['G-Retriever']*325\n",
    "\n",
    "ft_df['category'] = ['syntax'] * 56 + ['dependencies'] * 54 + ['meta'] * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(font_scale=1.2)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 6))\n",
    "\n",
    "\n",
    "ax1 = sns.barplot(g_retr_df, errorbar=('ci', 100), ax=ax[0])\n",
    "for con in ax1.containers:\n",
    "    ax1.bar_label(con, label_type='center', fontsize=11)\n",
    "ax1.set_ylabel(\"Average score\", fontsize=13)\n",
    "ax1.set_xlabel(\"(a) G-Retriever\", fontsize=13)\n",
    "\n",
    "ax2 = sns.barplot(rag_df, errorbar=('ci', 100), ax=ax[1])\n",
    "for con in ax2.containers:\n",
    "    ax2.bar_label(con, label_type='center', fontsize=11)\n",
    "ax2.set_xlabel(\"(b) RAG\", fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([rag_df, g_retr_df], axis=0)\n",
    "df_full = pd.melt(df_full, id_vars=['type', 'category'])\n",
    "\n",
    "print(df_full)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "sns.violinplot(df_full,\n",
    "               x='variable',\n",
    "               y='value',\n",
    "               hue=\"type\",\n",
    "               split=True, inner='quart', gridsize=50, ax=ax)\n",
    "ax.set_xlabel(\"Context window length\", fontsize=13)\n",
    "ax.set_ylabel(\"Average score\", fontsize=13)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(-0.14,1), fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(\n",
    "    pd.concat([rag_df, g_retr_df]).groupby(['type', 'category']).mean(),\n",
    "    annot=True\n",
    ")\n",
    "ax.set_ylabel(\"LLM type and question category\", fontsize=13)\n",
    "ax.set_xlabel(\"Context length\", fontsize=13)\n",
    "\n",
    "pd.concat([rag_df, g_retr_df]).groupby(['type', 'category']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.barplot(ft_df, errorbar=('ci', 100))\n",
    "for con in ax1.containers:\n",
    "    ax1.bar_label(con, label_type='center')\n",
    "ax1.set_ylabel(\"Average score\", fontsize=13)\n",
    "ax1.set_xlabel(\"Context length\", fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(\n",
    "    ft_df.groupby('category').mean(),\n",
    "    annot=True\n",
    ")\n",
    "ax.set_ylabel(\"Question category\", fontsize=13)\n",
    "ax.set_xlabel(\"Context length\", fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model vs Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "path = \"Model Outpus & Evaluation/Model vs Model Evaluation\"\n",
    "file_names = sorted(os.listdir(path))\n",
    "\n",
    "f_name_to_scores = dict()\n",
    "for fname in file_names:\n",
    "    with open(f\"{path}/{fname}\") as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(x)['verdict'] for x in data]\n",
    "\n",
    "    f_name_to_scores[fname] = dict(Counter(data))\n",
    "\n",
    "# Collecting the data in the format:\n",
    "# <RAG is better>, <G-Retriever is better>, <Both assistants are good>, <Both assistants are bad>\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"type\", \"RAG\", \"G-Retriever\", \"Tie good\", \"Tie bad\"])\n",
    "\n",
    "for i in range(0, 14, 2):\n",
    "    type = file_names[i].split()[1]\n",
    "\n",
    "    g_retr_vs_rag = f_name_to_scores[file_names[i]]\n",
    "    rag_vs_g_retr = f_name_to_scores[file_names[i+1]]\n",
    "\n",
    "    num_scores = sum(list(g_retr_vs_rag.values()))\n",
    "\n",
    "    g_retr   = 100 * ( g_retr_vs_rag[\"Assistant A's answer is better\"] + rag_vs_g_retr[\"Assistant B's answer is better\"] ) / (2*num_scores)\n",
    "    rag      = 100 * ( g_retr_vs_rag[\"Assistant B's answer is better\"] + rag_vs_g_retr[\"Assistant A's answer is better\"] ) / (2*num_scores)\n",
    "    tie_good = 100 * ( g_retr_vs_rag[\"Both assistants are good\"] + rag_vs_g_retr[\"Both assistants are good\"] ) / (2*num_scores)\n",
    "    tie_bad  = 100 * ( g_retr_vs_rag[\"Both assistants are bad\"] + rag_vs_g_retr[\"Both assistants are bad\"] ) / (2*num_scores)\n",
    "\n",
    "    results_df.loc[len(results_df)] = (type, rag, g_retr, tie_good, tie_bad)\n",
    "\n",
    "results_df = results_df.melt('type')\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(30, 6))\n",
    "sns.barplot(results_df, x=\"type\", y=\"value\", hue=\"variable\", ax=ax)\n",
    "\n",
    "for con in ax.containers:\n",
    "    ax.bar_label(con, fontsize=11.5)\n",
    "\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='18') # for legend title\n",
    "\n",
    "ax.set_ylabel(\"Average % winrate\", fontsize=15)\n",
    "ax.set_xlabel(\"Context length\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ft_df = pd.DataFrame(columns=[\"type\", \"Other model\", \"Finetuned G-Retriever\", \"Tie good\", \"Tie bad\"])\n",
    "\n",
    "descriptions = [\n",
    "    \"(a) - comparison with RAG, context length = 128\",\n",
    "    \"(b) - comparison with RAG, context length = 512\",\n",
    "    \"(c) - comparison with G-Retriever, context length = 128\",\n",
    "    \"(d) - comparison with G-Retriever, context length = 512\",\n",
    "]\n",
    "\n",
    "for i, description in zip(range(14, 24, 2), descriptions):\n",
    "    rag_vs_g_retr = f_name_to_scores[file_names[i]]\n",
    "\n",
    "    g_retr_vs_rag = f_name_to_scores[file_names[i+1]]\n",
    "\n",
    "    num_scores = sum(list(g_retr_vs_rag.values()))\n",
    "\n",
    "    g_retr   = 100 * ( g_retr_vs_rag[\"Assistant A's answer is better\"] + rag_vs_g_retr[\"Assistant B's answer is better\"] ) / (2*num_scores)\n",
    "    other    = 100 * ( g_retr_vs_rag[\"Assistant B's answer is better\"] + rag_vs_g_retr[\"Assistant A's answer is better\"] ) / (2*num_scores)\n",
    "    tie_good = 100 * ( g_retr_vs_rag[\"Both assistants are good\"]       + rag_vs_g_retr[\"Both assistants are good\"] )       / (2*num_scores)\n",
    "    tie_bad  = 100 * ( g_retr_vs_rag[\"Both assistants are bad\"]        + rag_vs_g_retr[\"Both assistants are bad\"] )        / (2*num_scores)\n",
    "\n",
    "    results_ft_df.loc[len(results_ft_df)] = (description, other, g_retr, tie_good, tie_bad)\n",
    "\n",
    "results_ft_df = results_ft_df.melt('type')\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(30, 6))\n",
    "sns.barplot(results_ft_df, x=\"type\", y=\"value\", hue=\"variable\", ax=ax)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='12') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='15') # for legend title\n",
    "ax.set_ylabel(\"Average % winrate\", fontsize=15)\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "for con in ax.containers:\n",
    "    ax.bar_label(con, fontsize=13)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(-0.14, 1.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ft_df = pd.DataFrame(columns=[\"type\", \"Context length = 128\", \"Context length = 512\", \"Tie good\", \"Tie bad\"])\n",
    "\n",
    "rag_vs_g_retr = f_name_to_scores[file_names[-2]]\n",
    "g_retr_vs_rag = f_name_to_scores[file_names[-1]]\n",
    "\n",
    "num_scores = sum(list(g_retr_vs_rag.values()))\n",
    "\n",
    "g_retr   = 100 * ( g_retr_vs_rag[\"Assistant A's answer is better\"] + rag_vs_g_retr[\"Assistant B's answer is better\"] ) / (2*num_scores)\n",
    "other    = 100 * ( g_retr_vs_rag[\"Assistant B's answer is better\"] + rag_vs_g_retr[\"Assistant A's answer is better\"] ) / (2*num_scores)\n",
    "tie_good = 100 * ( g_retr_vs_rag[\"Both assistants are good\"]       + rag_vs_g_retr[\"Both assistants are good\"] )       / (2*num_scores)\n",
    "tie_bad  = 100 * ( g_retr_vs_rag[\"Both assistants are bad\"]        + rag_vs_g_retr[\"Both assistants are bad\"] )        / (2*num_scores)\n",
    "\n",
    "results_ft_df.loc[len(results_ft_df)] = (\"\", other, g_retr, tie_good, tie_bad)\n",
    "\n",
    "results_ft_df = results_ft_df.melt('type')\n",
    "\n",
    "# f, ax = plt.subplots(1, 1, figsize=(30, 6))\n",
    "ax = sns.barplot(results_ft_df, x=\"type\", y=\"value\", hue=\"variable\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Average % winrate\")\n",
    "\n",
    "for con in ax.containers:\n",
    "    ax.bar_label(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model vs Model with question categories (Syntax, Dependencies, and Meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"Model Outpus & Evaluation/Model vs Model Evaluation\"\n",
    "file_names = sorted(os.listdir(path))\n",
    "\n",
    "f_name_to_scores = dict()\n",
    "for fname in file_names:\n",
    "    with open(f\"{path}/{fname}\") as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(x)['verdict'] for x in data]\n",
    "\n",
    "    f_name_to_scores[fname] = data\n",
    "\n",
    "# Collecting the data in the format:\n",
    "# <RAG is better>, <G-Retriever is better>, <Both assistants are good>, <Both assistants are bad>\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Type\", \"Category\", \"RAG\", \"G-Retriever\", \"Tie good\", \"Tie bad\"])\n",
    "categories = [\"Syntax\", \"Dependencies\", \"Meta\"]\n",
    "slices = [slice(0, 140), slice(140, 275), slice(275, 325)]\n",
    "\n",
    "for i in range(0, 14, 2):\n",
    "    type = file_names[i].split()[1]\n",
    "\n",
    "    for cat, sli in zip(categories, slices):\n",
    "        \n",
    "        g_retr_vs_rag = f_name_to_scores[file_names[i]][sli]\n",
    "        rag_vs_g_retr = f_name_to_scores[file_names[i+1]][sli]\n",
    "\n",
    "        g_retr_vs_rag = dict(Counter(g_retr_vs_rag))\n",
    "        rag_vs_g_retr = dict(Counter(rag_vs_g_retr))\n",
    "\n",
    "        num_scores = sum(list(g_retr_vs_rag.values()))\n",
    "\n",
    "        g_retr   = 100 * ( g_retr_vs_rag[\"Assistant A's answer is better\"] + rag_vs_g_retr[\"Assistant B's answer is better\"] ) / (2*num_scores)\n",
    "        rag      = 100 * ( g_retr_vs_rag[\"Assistant B's answer is better\"] + rag_vs_g_retr[\"Assistant A's answer is better\"] ) / (2*num_scores)\n",
    "        tie_good = 100 * ( g_retr_vs_rag[\"Both assistants are good\"] + rag_vs_g_retr[\"Both assistants are good\"] ) / (2*num_scores)\n",
    "        tie_bad  = 100 * ( g_retr_vs_rag[\"Both assistants are bad\"] + rag_vs_g_retr[\"Both assistants are bad\"] ) / (2*num_scores)\n",
    "\n",
    "        results_df.loc[len(results_df)] = (int(type), cat, rag, g_retr, tie_good, tie_bad)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "pl, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    sns.heatmap(\n",
    "        results_df.loc[results_df.Category == cat].set_index('Type').drop(['Category'], axis=1),\n",
    "        annot=True, ax=ax[i])\n",
    "    ax[i].set_ylabel(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ft_df = pd.DataFrame(columns=[\"Type\", \"Category\", \"Other model\", \"Finetuned G-Retriever\", \"Tie good\", \"Tie bad\"])\n",
    "\n",
    "descriptions = [\n",
    "    \"(a) - comparison with RAG,\\ncontext length = 128\",\n",
    "    \"(b) - comparison with RAG,\\ncontext length = 512\",\n",
    "    \"(c) - comparison with G-Retriever,\\ncontext length = 128\",\n",
    "    \"(d) - comparison with G-Retriever,\\ncontext length = 512\",\n",
    "]\n",
    "\n",
    "categories = [\"Syntax\", \"Dependencies\", \"Meta\"]\n",
    "slices = [slice(0, 56), slice(56, 110), slice(110, 130)]\n",
    "\n",
    "for i, description in zip(range(14, 24, 2), descriptions):\n",
    "    \n",
    "    for cat, sli in zip(categories, slices):\n",
    "        rag_vs_g_retr = f_name_to_scores[file_names[i]][sli]\n",
    "        g_retr_vs_rag = f_name_to_scores[file_names[i+1]][sli]\n",
    "\n",
    "        rag_vs_g_retr = dict(Counter(rag_vs_g_retr))\n",
    "        g_retr_vs_rag = dict(Counter(g_retr_vs_rag))\n",
    "\n",
    "        num_scores = sum(list(g_retr_vs_rag.values()))\n",
    "\n",
    "        g_retr   = 100 * ( g_retr_vs_rag.get(\"Assistant A's answer is better\", 0) + rag_vs_g_retr[\"Assistant B's answer is better\"] ) / (2*num_scores)\n",
    "        other    = 100 * ( g_retr_vs_rag[\"Assistant B's answer is better\"] + rag_vs_g_retr.get(\"Assistant A's answer is better\", 0) ) / (2*num_scores)\n",
    "        tie_good = 100 * ( g_retr_vs_rag[\"Both assistants are good\"]       + rag_vs_g_retr[\"Both assistants are good\"] ) / (2*num_scores)\n",
    "        tie_bad  = 100 * ( g_retr_vs_rag[\"Both assistants are bad\"]        + rag_vs_g_retr[\"Both assistants are bad\"] ) / (2*num_scores)\n",
    "\n",
    "        results_ft_df.loc[len(results_ft_df)] = (description, cat, other, g_retr, tie_good, tie_bad)\n",
    "\n",
    "results_ft_df\n",
    "\n",
    "p, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    df_to_plot = results_ft_df.loc[results_ft_df.Category == cat].drop([\"Category\"], axis=1).set_index(\"Type\")\n",
    "    if i == 0: sns.heatmap(df_to_plot,ax=ax[i], annot=True)\n",
    "    else:      sns.heatmap(df_to_plot,ax=ax[i], annot=True, yticklabels=[])\n",
    "    ax[i].set_ylabel(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ft_df = pd.DataFrame(columns=[\"Type\", \"Category\", \"Context length = 128\", \"Context length = 512\", \"Tie good\", \"Tie bad\"])\n",
    "\n",
    "for cat, sli in zip(categories, slices):\n",
    "    rag_vs_g_retr = f_name_to_scores[file_names[-2]][sli]\n",
    "    g_retr_vs_rag = f_name_to_scores[file_names[-1]][sli]\n",
    "\n",
    "    rag_vs_g_retr = dict(Counter(rag_vs_g_retr))\n",
    "    g_retr_vs_rag = dict(Counter(g_retr_vs_rag))\n",
    "\n",
    "    num_scores = sum(list(g_retr_vs_rag.values()))\n",
    "\n",
    "    g_retr   = 100 * ( g_retr_vs_rag[\"Assistant A's answer is better\"] + rag_vs_g_retr[\"Assistant B's answer is better\"] ) / (2*num_scores)\n",
    "    other    = 100 * ( g_retr_vs_rag[\"Assistant B's answer is better\"] + rag_vs_g_retr[\"Assistant A's answer is better\"] ) / (2*num_scores)\n",
    "    tie_good = 100 * ( g_retr_vs_rag[\"Both assistants are good\"]       + rag_vs_g_retr[\"Both assistants are good\"] )       / (2*num_scores)\n",
    "    tie_bad  = 100 * ( g_retr_vs_rag[\"Both assistants are bad\"]        + rag_vs_g_retr[\"Both assistants are bad\"] )        / (2*num_scores)\n",
    "\n",
    "    results_ft_df.loc[len(results_ft_df)] = (\"\", cat, other, g_retr, tie_good, tie_bad)\n",
    "\n",
    "results_ft_df\n",
    "\n",
    "p, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "results_ft_df = results_ft_df.set_index('Category').drop([\"Type\"], axis=1)\n",
    "sns.heatmap(results_ft_df, annot=True, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
